{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class LinearActivation:\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "    def backward(self, output_gradient):\n",
        "        return output_gradient\n",
        "\n",
        "class ReLU:\n",
        "    def forward(self, x):\n",
        "        self.input = x\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def backward(self, output_gradient):\n",
        "        return output_gradient * (self.input > 0)\n",
        "\n",
        "class Sigmoid:\n",
        "    def forward(self, x):\n",
        "        self.output = 1 / (1 + np.exp(-x))\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, output_gradient):\n",
        "        return output_gradient * (self.output * (1 - self.output))\n",
        "\n",
        "class Tanh:\n",
        "    def forward(self, x):\n",
        "        self.output = np.tanh(x)\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, output_gradient):\n",
        "        return output_gradient * (1 - self.output ** 2)\n",
        "\n",
        "class Softmax:\n",
        "    def forward(self, x):\n",
        "        exps = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "        self.output = exps / np.sum(exps, axis=-1, keepdims=True)\n",
        "        return self.output\n",
        "\n",
        "class Layer:\n",
        "    def __init__(self, input_dim, output_dim, activation=None):\n",
        "        self.weights = np.random.randn(input_dim, output_dim) * 0.1\n",
        "        self.bias = np.zeros(output_dim)\n",
        "        self.activation = activation() if activation else None\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        self.input = input_data\n",
        "        self.z = np.dot(input_data, self.weights) + self.bias\n",
        "        self.output = self.activation.forward(self.z) if self.activation else self.z\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, output_gradient):\n",
        "        \"\"\"Computes gradients and returns input gradient for next layer (previous in terms of backprop)\"\"\"\n",
        "        activation_grad = self.activation.backward(output_gradient) if self.activation else output_gradient\n",
        "        self.weights_gradient = np.dot(self.input.T, activation_grad)\n",
        "        self.bias_gradient = np.sum(activation_grad, axis=0)\n",
        "        return np.dot(activation_grad, self.weights.T)\n",
        "\n",
        "    def update_params(self, learning_rate):\n",
        "        \"\"\"Updates weights and biases using the computed gradients\"\"\"\n",
        "        self.weights -= learning_rate * self.weights_gradient\n",
        "        self.bias -= learning_rate * self.bias_gradient\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "\n",
        "    def add_layer(self, input_dim, output_dim, activation=None):\n",
        "        self.layers.append(Layer(input_dim, output_dim, activation))\n",
        "\n",
        "    def predict(self, input_data):\n",
        "        for layer in self.layers:\n",
        "            input_data = layer.forward(input_data)\n",
        "        return input_data\n",
        "\n",
        "    def train(self, X, Y, loss_fn, learning_rate=0.01):\n",
        "        predictions = self.predict(X)\n",
        "        loss = loss_fn.forward(predictions, Y)\n",
        "        loss_grad = loss_fn.backward(predictions, Y)\n",
        "        for layer in reversed(self.layers):\n",
        "            loss_grad = layer.backward(loss_grad)\n",
        "        for layer in self.layers:\n",
        "            layer.update_params(learning_rate)\n",
        "        return loss\n",
        "\n",
        "class CrossEntropyLoss:\n",
        "    def forward(self, predictions, labels):\n",
        "        self.predictions = predictions\n",
        "        self.labels = labels\n",
        "        return -np.sum(labels * np.log(predictions + 1e-9)) / predictions.shape[0]\n",
        "\n",
        "    def backward(self, predictions, labels):\n",
        "        # Directly return the gradient for softmax with cross-entropy\n",
        "        # This simplification works because dL/dz = y_pred - y_true for softmax + cross-entropy\n",
        "        return predictions - labels\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    nn = NeuralNetwork()\n",
        "    nn.add_layer(input_dim=4, output_dim=5, activation=ReLU)\n",
        "    nn.add_layer(input_dim=5, output_dim=3, activation=Sigmoid)\n",
        "\n",
        "    X = np.array([[0.1, 0.2, 0.3, 0.4], [0.1, 0.2, 0.3, 0.4]])\n",
        "    Y = np.array([[1, 0, 0], [0, 1, 0]])\n",
        "\n",
        "    loss_fn = CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(1000):\n",
        "        loss = nn.train(X, Y, loss_fn, learning_rate=0.01)\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aXsDFaNUGAL",
        "outputId": "6585b204-37b7-4bb3-edc3-a0f30d8d4488"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.6930666078767866\n",
            "Epoch 100, Loss: 0.692186232442549\n",
            "Epoch 200, Loss: 0.6918591111226097\n",
            "Epoch 300, Loss: 0.6915140894350671\n",
            "Epoch 400, Loss: 0.6912341959759591\n",
            "Epoch 500, Loss: 0.6910135049084263\n",
            "Epoch 600, Loss: 0.6908470424355988\n",
            "Epoch 700, Loss: 0.6907298954262662\n",
            "Epoch 800, Loss: 0.6906569650214403\n",
            "Epoch 900, Loss: 0.6906229894342951\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yKZiPBUSVEBO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}